 * Serving Flask app 'web_app'
 * Debug mode: on
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:werkzeug: * Restarting with watchdog (windowsapi)
Traceback (most recent call last):
  File "d:\bitcoinproject\Bitcoin_1-main\bitcoin2\web_app.py", line 18, in <module>
    session = ort.InferenceSession('model/rf_model.onnx')
  File "C:\Users\spand\AppData\Roaming\Python\Python313\site-packages\onnxruntime\capi\onnxruntime_inference_collection.py", line 485, in __init__
    self._create_inference_session(providers, provider_options, disabled_optimizers)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\spand\AppData\Roaming\Python\Python313\site-packages\onnxruntime\capi\onnxruntime_inference_collection.py", line 573, in _create_inference_session
    sess = C.InferenceSession(session_options, self._model_path, True, self._read_config_from_model)
onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Load model from model/rf_model.onnx failed:bad allocation
